{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Capstone - Classification Experiment.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nicolenair/capstone/blob/master/Capstone_Classification_Experiment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YlL6OZZYuLPv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "# !pip install importlib\n",
        "# !pip install imp"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sb12I3GvV_Gm",
        "colab_type": "code",
        "outputId": "a7d65f0f-e381-4dcb-d4e9-af080a9e0812",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "os.chdir(\"/content/gdrive/My Drive/Colab Notebooks\")"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "taaK2gE9Toa-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!pip install import_ipynb\n",
        "\n",
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import re\n",
        "import os\n",
        "import importlib"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "96XXzcW-V-EQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# importlib.reload(anno_auto)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kusdyuiaX_YY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import os\n",
        "# os.listdir(\"/content/gdrive/My Drive/Nicole-Internship-Complete-Folder/chatbot - all/chatbot-flask/GoogleNews-vectors-negative300.bin.gz\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8xuNYM3QuLPz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "books = [\"gables\", \"dreams\", \"island\", \"avonlea\", \"poplars\" ,\"ingleside\", \"rainbow\", \"rilla\"]\n",
        "# urls = [\"https://www.gutenberg.org/files/45/45-0.txt\", \"https://www.gutenberg.org/files/47/47-0.txt\", \"https://www.gutenberg.org/files/51/51-0.txt\", \"http://www.gutenberg.org/cache/epub/544/pg544.txt\", \"http://www.gutenberg.org/cache/epub/3796/pg3796.txt\", \"http://gutenberg.net.au/ebooks01/0100281.txt\", \"http://gutenberg.net.au/ebooks01/0100251.txt\", \"http://www.gutenberg.org/files/5343/5343-0.txt\"]\n",
        "\n",
        "# for i in range(len(books)):\n",
        "#     page = requests.get(urls[i])\n",
        "#     file = open(\"{}.txt\".format(books[i]) , \"w+\")\n",
        "#     file.write(page.text)\n",
        "#     file.close()\n",
        "\n",
        "\n",
        "\n",
        "file = open(\"gables.txt\", \"r\")\n",
        "gables = file.read()\n",
        "file = open(\"avonlea.txt\", \"r\")\n",
        "avonlea = file.read()\n",
        "file = open(\"island.txt\", \"r\")\n",
        "island = file.read()\n",
        "file = open(\"dreams.txt\", \"r\")\n",
        "dreams = file.read()\n",
        "file = open(\"rilla.txt\", \"r\")\n",
        "rilla = file.read()\n",
        "file = open(\"ingleside.txt\", \"r\")\n",
        "ingleside = file.read()\n",
        "file = open(\"poplars.txt\", \"r\")\n",
        "poplars = file.read()\n",
        "file = open(\"rainbow.txt\", \"r\")\n",
        "rainbow = file.read()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "epf2i-IQuLP1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import gensim\n",
        "# must download & load for word2vec_average_doc function to work\n",
        "# # Load Google's pre-trained Word2Vec model.\n",
        "# model = gensim.models.KeyedVectors.load_word2vec_format('./GoogleNews-vectors-negative300.bin', binary=True)  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IOTOrUDjuLP4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# from gensim.parsing.preprocessing import preprocess_documents\n",
        "# from gensim.models import KeyedVectors\n",
        "# from gensim.models import Word2Vec\n",
        "\n",
        "# #returns vector representation of documents \n",
        "# def word2vec_average_doc(documents):\n",
        "#     model_new = Word2Vec(size=300, min_count=1)\n",
        "#     model_new.build_vocab(documents)\n",
        "#     total_examples = model_new.corpus_count\n",
        "#     model_new.build_vocab([list(model_new.wv.vocab.keys())], update=True) #fishy here, changed model to model_new within list bracket\n",
        "#     model_new.intersect_word2vec_format(\"/content/gdrive/My Drive/Nicole-Internship-Complete-Folder/chatbot - all/chatbot-flask/GoogleNews-vectors-negative300.bin.gz\", binary = True)\n",
        "#     model_new.train(documents, total_examples=total_examples, epochs=model_new.iter)\n",
        "#     mean_vectors = []\n",
        "#     processed = preprocess_documents(documents)\n",
        "#     for i in processed:\n",
        "#         summation = []\n",
        "#         for e in i:\n",
        "#             try:\n",
        "#                 summation.append(model_new[e])\n",
        "#             except KeyError:\n",
        "#                 #print(\"debug\")\n",
        "#                 next\n",
        "#         if len(summation)!=0:\n",
        "#             mean_vectors.append(np.mean(summation, axis = 0))\n",
        "#         else:\n",
        "#             mean_vectors.append(np.mean(mean_vectors, axis = 0))\n",
        "        \n",
        "#         #print(summation)\n",
        "#     return np.array(mean_vectors)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xcnPuUeyNbjs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "file1 = open(\"gables_sentence_list.txt\", \"r\")\n",
        "file2 = open(\"dreams_sentence_list.txt\", \"r\")\n",
        "file3 = open(\"island_sentence_list.txt\", \"r\")\n",
        "file4 = open(\"avonlea_sentence_list.txt\", \"r\")\n",
        "file5 = open(\"poplars_sentence_list.txt\", \"r\")\n",
        "file6 = open(\"ingleside_sentence_list.txt\", \"r\")\n",
        "file7 = open(\"rainbow_sentence_list.txt\", \"r\")\n",
        "file8 = open(\"rilla_sentence_list.txt\", \"r\")\n",
        "sentence_gables = []\n",
        "sentence_dreams = []\n",
        "sentence_island = []\n",
        "sentence_avonlea = []\n",
        "sentence_poplars = []\n",
        "sentence_ingleside = []\n",
        "sentence_rainbow = []\n",
        "sentence_rilla = []\n",
        "target_gables = []\n",
        "target_dreams = []\n",
        "target_island = []\n",
        "target_avonlea = [] ###\n",
        "target_poplars = []\n",
        "target_ingleside = []\n",
        "target_rainbow = []\n",
        "target_rilla = [] ###\n",
        "\n",
        "file1_lines =file1.readlines()\n",
        "file2_lines = file2.readlines()\n",
        "file3_lines = file3.readlines()\n",
        "file4_lines = file4.readlines()\n",
        "file5_lines = file5.readlines()\n",
        "file6_lines = file6.readlines()\n",
        "file7_lines = file7.readlines()\n",
        "file8_lines = file8.readlines()\n",
        "\n",
        "files = [file1_lines, file2_lines, file3_lines ,file4_lines, file5_lines, file6_lines, file7_lines, file8_lines]\n",
        "sentences = [sentence_gables, sentence_dreams, sentence_island, sentence_avonlea, sentence_poplars, sentence_ingleside, sentence_rainbow, sentence_rilla]\n",
        "targets = [target_gables, target_dreams, target_island, target_avonlea, target_poplars, target_ingleside, target_rainbow, target_rilla]\n",
        "\n",
        "for s in range(len(sentences)):\n",
        "  for i in range(len(files[s])):\n",
        "    if i%2==0:\n",
        "      sentences[s].append([files[s][i]])\n",
        "    else:\n",
        "      targets[s].append([int(files[s][i])])\n",
        "  file.close()\n",
        "\n",
        "    \n",
        "# for i in range(len(file2_lines)):\n",
        "#   if i%2==0:\n",
        "#     sentence_dreams.append([file2_lines[i]])\n",
        "#   else:\n",
        "#     target_dreams.append([int(file2_lines[i])])\n",
        "# file.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pw_9unW9KW3E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "full_corpus = []\n",
        "\n",
        "for i in sentences:\n",
        "  full_corpus.extend(i)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6clEXYjTNoHT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from gensim.test.utils import common_texts\n",
        "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
        "\n",
        "documents = [TaggedDocument(doc, [i]) for i, doc in enumerate(full_corpus)]\n",
        "model_d2v = Doc2Vec(documents, vector_size=100, window=2, min_count=1, workers=4)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KQ7CsvBPO6I2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from scipy.sparse import csr_matrix\n",
        "from random import sample\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "\n",
        "#function for training classifier\n",
        "def train_classifier(full_para_list, target, vectorizer=model_d2v, prnt = True, d2v=False):\n",
        "    if d2v==False:\n",
        "  \n",
        "      vectorizer = CountVectorizer()\n",
        "      n=len(target)\n",
        "\n",
        "      x_lst = []\n",
        "      for i in full_para_list[:n]:\n",
        "          x_lst.extend(i) \n",
        "      vectorized = vectorizer.fit_transform(x_lst)\n",
        "      x = vectorized.todense()\n",
        "    else:\n",
        "      n = len(target)\n",
        "      x_lst = []\n",
        "      for i in full_para_list[:n]:\n",
        "        x_lst.extend(i)\n",
        "      vectorized = np.array([vectorizer.infer_vector(i.split()) for i in x_lst])\n",
        "      x = vectorized\n",
        "#     x = word2vec_average_doc(x_lst) \n",
        "#     x = model_d2v.infer_vector(x_lst)\n",
        "#     print(x)\n",
        "\n",
        "\n",
        "    y = []\n",
        "    for i in target[:n]:\n",
        "        y.extend(i) \n",
        "    y = [0 if i!=1 else 1 for i in y]\n",
        "    ind_0 = [e for e, i in enumerate(y) if i==0]\n",
        "    ind_1 = [e for e, i in enumerate(y) if i==1]\n",
        "#     print(ind_0)\n",
        "#     print(ind_1)\n",
        "    \n",
        "        \n",
        "    if len(ind_0)>len(ind_1):\n",
        "        \n",
        "        new_indices = random.sample(ind_0, len(ind_1))\n",
        "#         print(new_indices)\n",
        "        x = np.vstack([x[new_indices], x[ind_1]])\n",
        "        y = [0 for i in range(len(new_indices))] + [1 for i in range(len(ind_1))]\n",
        "    else:\n",
        "        new_indices = sample(ind_1, len(ind_0))\n",
        "        x = np.vstack([x[new_indices], x[ind_0]])\n",
        "        y = [0 for i in range(len(new_indices))] + [1 for i in range(len(ind_0))]\n",
        "\n",
        "#     print(x)\n",
        "    model2 = LogisticRegression(solver='lbfgs', multi_class='multinomial')\n",
        "   # X_train, X_test, y_train, y_test = train_test_split(x, y)\n",
        "    model2.fit(x, y)\n",
        "#     if prnt==True:\n",
        "#         print(model2.score(X_test, y_test))\n",
        "#         y_pred = model2.predict(X_test)\n",
        "#         print(precision_score(y_test, y_pred,  pos_label = 1))\n",
        "#         print(precision_score(y_test, y_pred,  pos_label = 0))\n",
        "#         print(recall_score(y_test, y_pred, pos_label = 1))\n",
        "#         print(recall_score(y_test, y_pred, pos_label = 0 ))\n",
        "#         print(confusion_matrix(y_test, y_pred))\n",
        "#         tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
        "#         print(tn, fp, fn, tp)\n",
        "    return model2, vectorizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rvqd8fwVuLQC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import random\n",
        "\n",
        "# for checking performance of trained classifiers on books\n",
        "def test(full_para_list, model2, vectorizer, target, d2v=False):\n",
        "    n = len(target)\n",
        "#     print(n)\n",
        "    x_lst = []\n",
        "    for i in full_para_list:\n",
        "        x_lst.extend(i) \n",
        "    if d2v!=True:\n",
        "      vectorized = vectorizer.transform(x_lst)\n",
        "      x = vectorized.todense()\n",
        "    else:\n",
        "      vectorized = [vectorizer.infer_vector(i.split()) for i in x_lst]\n",
        "      x = np.array(vectorized)\n",
        "\n",
        "    \n",
        "#     x = word2vec_average_doc(x_lst) \n",
        "#     x = vectorizer.infer_vector(x_lst)\n",
        "    y = []\n",
        "    for i in target:\n",
        "        y.extend(i) \n",
        "\n",
        "    y = [0 if i!=1 else 1 for i in y]\n",
        "    ind_0 = [e for e, i in enumerate(y) if i==0]\n",
        "    ind_1 = [e for e, i in enumerate(y) if i==1]\n",
        "\n",
        "#     print(ind_0)\n",
        "#     print(ind_1)\n",
        "    if len(ind_0)>len(ind_1):\n",
        "        new_indices = random.sample(ind_0, len(ind_1))\n",
        "        #print(x.shape)\n",
        "#         print(new_indices)\n",
        "        x = np.vstack([x[new_indices], x[ind_1]])\n",
        "        \n",
        "        y = [0 for i in range(len(new_indices))] + [1 for i in range(len(ind_1))]\n",
        "    else:\n",
        "        new_indices = sample(ind_1, len(ind_0))\n",
        "        x = np.vstack([x[new_indices], x[ind_0]])\n",
        "        y = [0 for i in range(len(new_indices))] + [1 for i in range(len(ind_0))]\n",
        "\n",
        "    print(model2.score(x, y))\n",
        "    y_pred = model2.predict(x)\n",
        "    print(\"precision, label = 1\")\n",
        "    print(precision_score(y, y_pred,  pos_label = 1))\n",
        "    print(\"precision, label = 0\")\n",
        "    print(precision_score(y, y_pred,  pos_label = 0))\n",
        "    print(\"recall, label = 1\")\n",
        "    print(recall_score(y, y_pred, pos_label = 1))\n",
        "    print(\"recall, label = 0\")\n",
        "    print(recall_score(y, y_pred, pos_label = 0 ))\n",
        "\n",
        "    print(confusion_matrix(y, y_pred))\n",
        "    tn, fp, fn, tp = confusion_matrix(y, y_pred).ravel()\n",
        "    print(tn, fp, fn, tp)\n",
        "    print()\n",
        "\n",
        "#     words = np.array(vectorizer.get_feature_names())\n",
        "#     words[np.argsort(abs(model2.coef_[0]))[-10:]], model2.coef_[0][abs(np.argsort(model2.coef_[0]))[-10:]]\n",
        "    \n",
        "# print(\"trained on AGG, tested on AGG (control)\")\n",
        "# model2, vectorizer = train_classifier(full_para_list_gables, anno.return_target(\"gables\"), prnt = False)\n",
        "# test(full_para_list_gables, model2, vectorizer, anno.return_target(\"gables\"))\n",
        "\n",
        "# print(\"trained on AGG, tested on AHD\")\n",
        "# model2, vectorizer = train_classifier(full_para_list_gables, anno.return_target(\"gables\"), prnt = False)\n",
        "# test(full_para_list_dreams, model2, vectorizer, anno.return_target(\"dreams\"))\n",
        "\n",
        "# print(\"trained on AHD, tested on AHD (control)\")\n",
        "# model2, vectorizer = train_classifier(full_para_list_dreams,anno.return_target(\"dreams\"), prnt = False)\n",
        "# test(full_para_list_dreams, model2, vectorizer, anno.return_target(\"dreams\"))\n",
        "\n",
        "# print(\"trained on AHD, tested on AGG\")\n",
        "# model2, vectorizer = train_classifier(full_para_list_dreams, anno.return_target(\"dreams\"), prnt = False)\n",
        "# test(full_para_list_gables, model2, vectorizer, anno.return_target(\"dreams\"))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "35aIY4lzn60h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# sentence_gables_Marilla, target_gables_Marilla = anno_auto.return_nsubj(\"gables\", \"Marilla\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PUj2gkvsaFby",
        "colab_type": "code",
        "outputId": "b87c4ec4-78d3-418a-eff3-177e51af817c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#as compared to other nsubj sentences\n",
        "\n",
        "\n",
        "for i in range(len(sentences)):\n",
        "  for e in range(len(sentences)):\n",
        "    X_train, X_test, y_train, y_test = train_test_split(sentences[i], targets[i])\n",
        "    if i!=e:\n",
        "      X_train_buff, X_test, y_train_buff, y_test = train_test_split(sentences[e], targets[e])\n",
        "\n",
        "    print(\"trained on {}, tested on {}\".format(books[i], books[e]))\n",
        "    model2, vectorizer = train_classifier(X_train, y_train, prnt = False)\n",
        "    test(X_test, model2, vectorizer, y_test)\n"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "trained on gables, tested on gables\n",
            "0.7663043478260869\n",
            "precision, label = 1\n",
            "0.7474747474747475\n",
            "precision, label = 0\n",
            "0.788235294117647\n",
            "recall, label = 1\n",
            "0.8043478260869565\n",
            "recall, label = 0\n",
            "0.7282608695652174\n",
            "[[67 25]\n",
            " [18 74]]\n",
            "67 25 18 74\n",
            "\n",
            "trained on gables, tested on dreams\n",
            "0.7755102040816326\n",
            "precision, label = 1\n",
            "0.7872340425531915\n",
            "precision, label = 0\n",
            "0.7647058823529411\n",
            "recall, label = 1\n",
            "0.7551020408163265\n",
            "recall, label = 0\n",
            "0.7959183673469388\n",
            "[[39 10]\n",
            " [12 37]]\n",
            "39 10 12 37\n",
            "\n",
            "trained on gables, tested on island\n",
            "0.7131147540983607\n",
            "precision, label = 1\n",
            "0.7166666666666667\n",
            "precision, label = 0\n",
            "0.7096774193548387\n",
            "recall, label = 1\n",
            "0.7049180327868853\n",
            "recall, label = 0\n",
            "0.7213114754098361\n",
            "[[44 17]\n",
            " [18 43]]\n",
            "44 17 18 43\n",
            "\n",
            "trained on gables, tested on avonlea\n",
            "0.7628205128205128\n",
            "precision, label = 1\n",
            "0.759493670886076\n",
            "precision, label = 0\n",
            "0.7662337662337663\n",
            "recall, label = 1\n",
            "0.7692307692307693\n",
            "recall, label = 0\n",
            "0.7564102564102564\n",
            "[[59 19]\n",
            " [18 60]]\n",
            "59 19 18 60\n",
            "\n",
            "trained on gables, tested on poplars\n",
            "0.6810344827586207\n",
            "precision, label = 1\n",
            "0.6666666666666666\n",
            "precision, label = 0\n",
            "0.6981132075471698\n",
            "recall, label = 1\n",
            "0.7241379310344828\n",
            "recall, label = 0\n",
            "0.6379310344827587\n",
            "[[37 21]\n",
            " [16 42]]\n",
            "37 21 16 42\n",
            "\n",
            "trained on gables, tested on ingleside\n",
            "0.6842105263157895\n",
            "precision, label = 1\n",
            "0.6721311475409836\n",
            "precision, label = 0\n",
            "0.6981132075471698\n",
            "recall, label = 1\n",
            "0.7192982456140351\n",
            "recall, label = 0\n",
            "0.6491228070175439\n",
            "[[37 20]\n",
            " [16 41]]\n",
            "37 20 16 41\n",
            "\n",
            "trained on gables, tested on rainbow\n",
            "0.625\n",
            "precision, label = 1\n",
            "1.0\n",
            "precision, label = 0\n",
            "0.5714285714285714\n",
            "recall, label = 1\n",
            "0.25\n",
            "recall, label = 0\n",
            "1.0\n",
            "[[4 0]\n",
            " [3 1]]\n",
            "4 0 3 1\n",
            "\n",
            "trained on gables, tested on rilla\n",
            "0.75\n",
            "precision, label = 1\n",
            "1.0\n",
            "precision, label = 0\n",
            "0.6666666666666666\n",
            "recall, label = 1\n",
            "0.5\n",
            "recall, label = 0\n",
            "1.0\n",
            "[[2 0]\n",
            " [1 1]]\n",
            "2 0 1 1\n",
            "\n",
            "trained on dreams, tested on gables\n",
            "0.7197802197802198\n",
            "precision, label = 1\n",
            "0.6851851851851852\n",
            "precision, label = 0\n",
            "0.7702702702702703\n",
            "recall, label = 1\n",
            "0.8131868131868132\n",
            "recall, label = 0\n",
            "0.6263736263736264\n",
            "[[57 34]\n",
            " [17 74]]\n",
            "57 34 17 74\n",
            "\n",
            "trained on dreams, tested on dreams\n",
            "0.6630434782608695\n",
            "precision, label = 1\n",
            "0.6190476190476191\n",
            "precision, label = 0\n",
            "0.7586206896551724\n",
            "recall, label = 1\n",
            "0.8478260869565217\n",
            "recall, label = 0\n",
            "0.4782608695652174\n",
            "[[22 24]\n",
            " [ 7 39]]\n",
            "22 24 7 39\n",
            "\n",
            "trained on dreams, tested on island\n",
            "0.6071428571428571\n",
            "precision, label = 1\n",
            "0.59375\n",
            "precision, label = 0\n",
            "0.625\n",
            "recall, label = 1\n",
            "0.6785714285714286\n",
            "recall, label = 0\n",
            "0.5357142857142857\n",
            "[[30 26]\n",
            " [18 38]]\n",
            "30 26 18 38\n",
            "\n",
            "trained on dreams, tested on avonlea\n",
            "0.6474358974358975\n",
            "precision, label = 1\n",
            "0.6185567010309279\n",
            "precision, label = 0\n",
            "0.6949152542372882\n",
            "recall, label = 1\n",
            "0.7692307692307693\n",
            "recall, label = 0\n",
            "0.5256410256410257\n",
            "[[41 37]\n",
            " [18 60]]\n",
            "41 37 18 60\n",
            "\n",
            "trained on dreams, tested on poplars\n",
            "0.6521739130434783\n",
            "precision, label = 1\n",
            "0.64\n",
            "precision, label = 0\n",
            "0.6666666666666666\n",
            "recall, label = 1\n",
            "0.6956521739130435\n",
            "recall, label = 0\n",
            "0.6086956521739131\n",
            "[[28 18]\n",
            " [14 32]]\n",
            "28 18 14 32\n",
            "\n",
            "trained on dreams, tested on ingleside\n",
            "0.7159090909090909\n",
            "precision, label = 1\n",
            "0.7714285714285715\n",
            "precision, label = 0\n",
            "0.6792452830188679\n",
            "recall, label = 1\n",
            "0.6136363636363636\n",
            "recall, label = 0\n",
            "0.8181818181818182\n",
            "[[36  8]\n",
            " [17 27]]\n",
            "36 8 17 27\n",
            "\n",
            "trained on dreams, tested on rainbow\n",
            "0.5\n",
            "precision, label = 1\n",
            "0.5\n",
            "precision, label = 0\n",
            "0.5\n",
            "recall, label = 1\n",
            "0.6666666666666666\n",
            "recall, label = 0\n",
            "0.3333333333333333\n",
            "[[1 2]\n",
            " [1 2]]\n",
            "1 2 1 2\n",
            "\n",
            "trained on dreams, tested on rilla\n",
            "1.0\n",
            "precision, label = 1\n",
            "1.0\n",
            "precision, label = 0\n",
            "1.0\n",
            "recall, label = 1\n",
            "1.0\n",
            "recall, label = 0\n",
            "1.0\n",
            "[[3 0]\n",
            " [0 3]]\n",
            "3 0 0 3\n",
            "\n",
            "trained on island, tested on gables\n",
            "0.7345679012345679\n",
            "precision, label = 1\n",
            "0.7021276595744681\n",
            "precision, label = 0\n",
            "0.7794117647058824\n",
            "recall, label = 1\n",
            "0.8148148148148148\n",
            "recall, label = 0\n",
            "0.654320987654321\n",
            "[[53 28]\n",
            " [15 66]]\n",
            "53 28 15 66\n",
            "\n",
            "trained on island, tested on dreams\n",
            "0.6509433962264151\n",
            "precision, label = 1\n",
            "0.6739130434782609\n",
            "precision, label = 0\n",
            "0.6333333333333333\n",
            "recall, label = 1\n",
            "0.5849056603773585\n",
            "recall, label = 0\n",
            "0.7169811320754716\n",
            "[[38 15]\n",
            " [22 31]]\n",
            "38 15 22 31\n",
            "\n",
            "trained on island, tested on island\n",
            "0.7936507936507936\n",
            "precision, label = 1\n",
            "0.7681159420289855\n",
            "precision, label = 0\n",
            "0.8245614035087719\n",
            "recall, label = 1\n",
            "0.8412698412698413\n",
            "recall, label = 0\n",
            "0.746031746031746\n",
            "[[47 16]\n",
            " [10 53]]\n",
            "47 16 10 53\n",
            "\n",
            "trained on island, tested on avonlea\n",
            "0.6941176470588235\n",
            "precision, label = 1\n",
            "0.6853932584269663\n",
            "precision, label = 0\n",
            "0.7037037037037037\n",
            "recall, label = 1\n",
            "0.7176470588235294\n",
            "recall, label = 0\n",
            "0.6705882352941176\n",
            "[[57 28]\n",
            " [24 61]]\n",
            "57 28 24 61\n",
            "\n",
            "trained on island, tested on poplars\n",
            "0.6022727272727273\n",
            "precision, label = 1\n",
            "0.6\n",
            "precision, label = 0\n",
            "0.6046511627906976\n",
            "recall, label = 1\n",
            "0.6136363636363636\n",
            "recall, label = 0\n",
            "0.5909090909090909\n",
            "[[26 18]\n",
            " [17 27]]\n",
            "26 18 17 27\n",
            "\n",
            "trained on island, tested on ingleside\n",
            "0.6428571428571429\n",
            "precision, label = 1\n",
            "0.6363636363636364\n",
            "precision, label = 0\n",
            "0.65\n",
            "recall, label = 1\n",
            "0.6666666666666666\n",
            "recall, label = 0\n",
            "0.6190476190476191\n",
            "[[26 16]\n",
            " [14 28]]\n",
            "26 16 14 28\n",
            "\n",
            "trained on island, tested on rainbow\n",
            "0.8333333333333334\n",
            "precision, label = 1\n",
            "0.75\n",
            "precision, label = 0\n",
            "1.0\n",
            "recall, label = 1\n",
            "1.0\n",
            "recall, label = 0\n",
            "0.6666666666666666\n",
            "[[2 1]\n",
            " [0 3]]\n",
            "2 1 0 3\n",
            "\n",
            "trained on island, tested on rilla\n",
            "0.5\n",
            "precision, label = 1\n",
            "0.5\n",
            "precision, label = 0\n",
            "0.5\n",
            "recall, label = 1\n",
            "0.5\n",
            "recall, label = 0\n",
            "0.5\n",
            "[[1 1]\n",
            " [1 1]]\n",
            "1 1 1 1\n",
            "\n",
            "trained on avonlea, tested on gables\n",
            "0.756578947368421\n",
            "precision, label = 1\n",
            "0.7294117647058823\n",
            "precision, label = 0\n",
            "0.7910447761194029\n",
            "recall, label = 1\n",
            "0.8157894736842105\n",
            "recall, label = 0\n",
            "0.6973684210526315\n",
            "[[53 23]\n",
            " [14 62]]\n",
            "53 23 14 62\n",
            "\n",
            "trained on avonlea, tested on dreams\n",
            "0.6481481481481481\n",
            "precision, label = 1\n",
            "0.6904761904761905\n",
            "precision, label = 0\n",
            "0.6212121212121212\n",
            "recall, label = 1\n",
            "0.5370370370370371\n",
            "recall, label = 0\n",
            "0.7592592592592593\n",
            "[[41 13]\n",
            " [25 29]]\n",
            "41 13 25 29\n",
            "\n",
            "trained on avonlea, tested on island\n",
            "0.753731343283582\n",
            "precision, label = 1\n",
            "0.7575757575757576\n",
            "precision, label = 0\n",
            "0.75\n",
            "recall, label = 1\n",
            "0.746268656716418\n",
            "recall, label = 0\n",
            "0.7611940298507462\n",
            "[[51 16]\n",
            " [17 50]]\n",
            "51 16 17 50\n",
            "\n",
            "trained on avonlea, tested on avonlea\n",
            "0.759493670886076\n",
            "precision, label = 1\n",
            "0.7204301075268817\n",
            "precision, label = 0\n",
            "0.8153846153846154\n",
            "recall, label = 1\n",
            "0.8481012658227848\n",
            "recall, label = 0\n",
            "0.6708860759493671\n",
            "[[53 26]\n",
            " [12 67]]\n",
            "53 26 12 67\n",
            "\n",
            "trained on avonlea, tested on poplars\n",
            "0.6979166666666666\n",
            "precision, label = 1\n",
            "0.7317073170731707\n",
            "precision, label = 0\n",
            "0.6727272727272727\n",
            "recall, label = 1\n",
            "0.625\n",
            "recall, label = 0\n",
            "0.7708333333333334\n",
            "[[37 11]\n",
            " [18 30]]\n",
            "37 11 18 30\n",
            "\n",
            "trained on avonlea, tested on ingleside\n",
            "0.6704545454545454\n",
            "precision, label = 1\n",
            "0.6666666666666666\n",
            "precision, label = 0\n",
            "0.6744186046511628\n",
            "recall, label = 1\n",
            "0.6818181818181818\n",
            "recall, label = 0\n",
            "0.6590909090909091\n",
            "[[29 15]\n",
            " [14 30]]\n",
            "29 15 14 30\n",
            "\n",
            "trained on avonlea, tested on rainbow\n",
            "0.25\n",
            "precision, label = 1\n",
            "0.3333333333333333\n",
            "precision, label = 0\n",
            "0.0\n",
            "recall, label = 1\n",
            "0.5\n",
            "recall, label = 0\n",
            "0.0\n",
            "[[0 2]\n",
            " [1 1]]\n",
            "0 2 1 1\n",
            "\n",
            "trained on avonlea, tested on rilla\n",
            "0.0\n",
            "precision, label = 1\n",
            "0.0\n",
            "precision, label = 0\n",
            "0.0\n",
            "recall, label = 1\n",
            "0.0\n",
            "recall, label = 0\n",
            "0.0\n",
            "[[0 1]\n",
            " [1 0]]\n",
            "0 1 1 0\n",
            "\n",
            "trained on poplars, tested on gables\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-4c86e1f4cdbf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"trained on {}, tested on {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbooks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbooks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mmodel2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvectorizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_classifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprnt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-061635b1f151>\u001b[0m in \u001b[0;36mtrain_classifier\u001b[0;34m(full_para_list, target, vectorizer, prnt, d2v)\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0mmodel2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msolver\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'lbfgs'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmulti_class\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'multinomial'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m    \u001b[0;31m# X_train, X_test, y_train, y_test = train_test_split(x, y)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     \u001b[0mmodel2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m \u001b[0;31m#     if prnt==True:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;31m#         print(model2.score(X_test, y_test))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m   1604\u001b[0m                       \u001b[0mpenalty\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpenalty\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_squared_sum\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_squared_sum\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1605\u001b[0m                       sample_weight=sample_weight)\n\u001b[0;32m-> 1606\u001b[0;31m             for class_, warm_start_coef_ in zip(classes_, warm_start_coef))\n\u001b[0m\u001b[1;32m   1607\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1608\u001b[0m         \u001b[0mfold_coefs_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_iter_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfold_coefs_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    919\u001b[0m             \u001b[0;31m# remaining jobs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    920\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 921\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    922\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    923\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    757\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    758\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 759\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    760\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    761\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    714\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    715\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 716\u001b[0;31m             \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    717\u001b[0m             \u001b[0;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    718\u001b[0m             \u001b[0;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    180\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    183\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    547\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    548\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 549\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    550\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    223\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 225\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    223\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 225\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py\u001b[0m in \u001b[0;36m_logistic_regression_path\u001b[0;34m(X, y, pos_class, Cs, fit_intercept, max_iter, tol, verbose, solver, coef, class_weight, dual, penalty, intercept_scaling, multi_class, random_state, check_input, max_squared_sum, sample_weight, l1_ratio)\u001b[0m\n\u001b[1;32m    942\u001b[0m                 \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfprime\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    943\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 944\u001b[0;31m                 iprint=iprint, pgtol=tol, maxiter=max_iter)\n\u001b[0m\u001b[1;32m    945\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"warnflag\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    946\u001b[0m                 warnings.warn(\"lbfgs failed to converge. Increase the number \"\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/scipy/optimize/lbfgsb.py\u001b[0m in \u001b[0;36mfmin_l_bfgs_b\u001b[0;34m(func, x0, fprime, args, approx_grad, bounds, m, factr, pgtol, epsilon, iprint, maxfun, maxiter, disp, callback, maxls)\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m     res = _minimize_lbfgsb(fun, x0, args=args, jac=jac, bounds=bounds,\n\u001b[0;32m--> 199\u001b[0;31m                            **opts)\n\u001b[0m\u001b[1;32m    200\u001b[0m     d = {'grad': res['jac'],\n\u001b[1;32m    201\u001b[0m          \u001b[0;34m'task'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'message'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/scipy/optimize/lbfgsb.py\u001b[0m in \u001b[0;36m_minimize_lbfgsb\u001b[0;34m(fun, x0, args, jac, bounds, disp, maxcor, ftol, gtol, eps, maxfun, maxiter, iprint, callback, maxls, **unknown_options)\u001b[0m\n\u001b[1;32m    333\u001b[0m             \u001b[0;31m# until the completion of the current minimization iteration.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m             \u001b[0;31m# Overwrite f and g:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 335\u001b[0;31m             \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc_and_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    336\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mtask_str\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mb'NEW_X'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    337\u001b[0m             \u001b[0;31m# new iteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/scipy/optimize/lbfgsb.py\u001b[0m in \u001b[0;36mfunc_and_grad\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    283\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mfunc_and_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 285\u001b[0;31m             \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    286\u001b[0m             \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjac\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/scipy/optimize/optimize.py\u001b[0m in \u001b[0;36mfunction_wrapper\u001b[0;34m(*wrapper_args)\u001b[0m\n\u001b[1;32m    324\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfunction_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mwrapper_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m         \u001b[0mncalls\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrapper_args\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    327\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mncalls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunction_wrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/scipy/optimize/optimize.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, x, *args)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m         \u001b[0mfg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjac\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x, *args)\u001b[0m\n\u001b[1;32m    917\u001b[0m         \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mY_multi\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    918\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msolver\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'lbfgs'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 919\u001b[0;31m             \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0m_multinomial_loss_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    920\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0msolver\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'newton-cg'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    921\u001b[0m             \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0m_multinomial_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py\u001b[0m in \u001b[0;36m_multinomial_loss_grad\u001b[0;34m(w, X, Y, alpha, sample_weight)\u001b[0m\n\u001b[1;32m    342\u001b[0m     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),\n\u001b[1;32m    343\u001b[0m                     dtype=X.dtype)\n\u001b[0;32m--> 344\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_multinomial_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    345\u001b[0m     \u001b[0msample_weight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m     \u001b[0mdiff\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample_weight\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py\u001b[0m in \u001b[0;36m_multinomial_loss\u001b[0;34m(w, X, Y, alpha, sample_weight)\u001b[0m\n\u001b[1;32m    290\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m         \u001b[0mintercept\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 292\u001b[0;31m     \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msafe_sparse_dot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    293\u001b[0m     \u001b[0mp\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mintercept\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m     \u001b[0mp\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0mlogsumexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/utils/extmath.py\u001b[0m in \u001b[0;36msafe_sparse_dot\u001b[0;34m(a, b, dense_output)\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dI-4FCemZv7i",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "bc907d28-b608-4241-c343-0bd693624850"
      },
      "source": [
        "len(gables)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "595661"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J6Bp0ON5cCOY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# #as compared to other nsubj sentences, using Marilla as comparison\n",
        "# X_train, X_test, y_train, y_test = train_test_split(sentence_gables, target_gables)\n",
        "# X_train_dreams, X_test_dreams, y_train_dreams, y_test_dreams = train_test_split(sentence_dreams, target_dreams)\n",
        "\n",
        "# print(\"trained on AGG, tested on AGG\")\n",
        "# model2, vectorizer = train_classifier(X_train, y_train, prnt = False)\n",
        "# test(sentence_gables, model2, vectorizer, target_gables)\n",
        "\n",
        "# print(\"trained on AGG, tested on AGG (control)(Marilla)\")\n",
        "# model2, vectorizer = train_classifier(X_train, y_train, prnt = False)\n",
        "# test(sentence_gables_Marilla, model2, vectorizer, target_gables_Marilla)\n",
        "\n",
        "# print(\"trained on AHD, tested on AGG(Marilla) (control)\")\n",
        "# model2, vectorizer = train_classifier(X_train_dreams, y_train_dreams, prnt = False)\n",
        "# test(sentence_gables_Marilla, model2, vectorizer, target_gables_Marilla)\n",
        "\n",
        "# print(\"trained on AHD, tested on AHD\")\n",
        "# model2, vectorizer = train_classifier(X_train_dreams, y_train_dreams, prnt = False)\n",
        "# test(X_test_dreams, model2, vectorizer, y_test_dreams)\n",
        "\n",
        "# # print(\"trained on AGG, tested on AHD\")\n",
        "# # model2, vectorizer = train_classifier(X_train, y_train, prnt = False)\n",
        "# # test(X_test_dreams, model2, vectorizer, y_test_dreams)\n",
        "\n",
        "# # print(\"trained on AHD, tested on AHD (control)\")\n",
        "# # model2, vectorizer = train_classifier(X_train_dreams,y_train_dreams, prnt = False)\n",
        "# # test(X_test_dreams, model2, vectorizer, y_test_dreams)\n",
        "\n",
        "# # print(\"trained on AHD, tested on AGG\")\n",
        "# # model2, vectorizer = train_classifier(X_train_dreams,y_train_dreams, prnt = False)\n",
        "# # test(X_test, model2, vectorizer, y_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0M1gLbkFvx47",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rGLFku_EuLQL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import networkx as nx\n",
        "\n",
        "#function for visualizing character networks in a book\n",
        "def gr(full_para_list):\n",
        "    a = np.zeros((20, 20))\n",
        "\n",
        "    x_lst = []\n",
        "    for i in full_para_list:\n",
        "        x_lst.extend(i) \n",
        "    # vectorized = vectorizer.transform(x_lst)\n",
        "\n",
        "    for i in x_lst:\n",
        "        chars = name_mapping.keys()\n",
        "        dis_chars = []\n",
        "        for ch in chars:\n",
        "            if re.search(ch, i) and ch not in dis_chars:\n",
        "                dis_chars.append(ch)\n",
        "        for ch in dis_chars:\n",
        "            for neighbor in dis_chars:\n",
        "                a[name_mapping[ch]][name_mapping[neighbor]] += 1\n",
        "\n",
        "    g1 = (nx.from_numpy_matrix((a- min(a.flatten()))/(max(a.flatten()) - min(a.flatten()))))\n",
        "    #g1 = (nx.from_numpy_matrix((a)\n",
        "    pos = nx.spring_layout(g1, k =1)\n",
        "    weights = [g1[u][v]['weight'] for u,v in g1.edges]\n",
        "    nx.draw(g1, pos = pos, width = weights)\n",
        "    nx.draw_networkx_labels(g1, labels = dict(map(reversed, name_mapping.items())), pos = pos)\n",
        "    plt.show()\n",
        "    return a\n",
        "    \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fFXoJ-z9uLQN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# a_gables = gr(full_para_list_gables)\n",
        "# a_dreams = gr(full_para_list_dreams)\n",
        "\n",
        "# #printing co-occurrence values for Anne with Diana & Gilbert in AGG\n",
        "# print(a_gables[1][6], a_gables[1][4])\n",
        "# #printing co-occurrence values for Anne with Cornelia & Gilbert in AHD\n",
        "# print(a_dreams[1][15], a_dreams[1][4])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SW8UXomPb96d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# train_classifier(full_para_list_gables, target_gables, prnt = False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zO7bf5RiOt2K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import anno"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}