{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "anno_auto.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nicolenair/capstone/blob/master/anno_auto.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2VmAOqYaqiHL",
        "colab_type": "code",
        "outputId": "3241656e-90e9-44a9-f95b-60cc40abd29f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 247
        }
      },
      "source": [
        "!pip install stanfordnlp\n",
        "!pip install import_ipynb\n"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: stanfordnlp in /usr/local/lib/python3.6/dist-packages (0.2.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from stanfordnlp) (4.28.1)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.6/dist-packages (from stanfordnlp) (3.7.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from stanfordnlp) (2.21.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from stanfordnlp) (1.16.5)\n",
            "Requirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from stanfordnlp) (1.2.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf->stanfordnlp) (41.2.0)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.6/dist-packages (from protobuf->stanfordnlp) (1.12.0)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->stanfordnlp) (2.8)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->stanfordnlp) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->stanfordnlp) (2019.9.11)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->stanfordnlp) (3.0.4)\n",
            "Requirement already satisfied: import_ipynb in /usr/local/lib/python3.6/dist-packages (0.1.3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WZil2jNymkHh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UIVqPybhrf-E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import re\n",
        "import stanfordnlp\n",
        "import import_ipynb"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ivTiAhxemm7r",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 730
        },
        "outputId": "bd6bbdbc-2d19-4a0b-997f-acc831b648a7"
      },
      "source": [
        "stanfordnlp.download('en')   # This downloads the English models for the neural pipeline\n",
        "nlp = stanfordnlp.Pipeline() # This sets up a default neural pipeline in English"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using the default treebank \"en_ewt\" for language \"en\".\n",
            "Would you like to download the models for: en_ewt now? (Y/n)\n",
            "y\n",
            "\n",
            "Default download directory: /root/stanfordnlp_resources\n",
            "Hit enter to continue or type an alternate directory.\n",
            "\n",
            "\n",
            "Downloading models for: en_ewt\n",
            "Download location: /root/stanfordnlp_resources/en_ewt_models.zip\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 235M/235M [00:13<00:00, 15.9MB/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Download complete.  Models saved to: /root/stanfordnlp_resources/en_ewt_models.zip\n",
            "Extracting models file for: en_ewt\n",
            "Cleaning up...Done.\n",
            "Use device: cpu\n",
            "---\n",
            "Loading: tokenize\n",
            "With settings: \n",
            "{'model_path': '/root/stanfordnlp_resources/en_ewt_models/en_ewt_tokenizer.pt', 'lang': 'en', 'shorthand': 'en_ewt', 'mode': 'predict'}\n",
            "---\n",
            "Loading: pos\n",
            "With settings: \n",
            "{'model_path': '/root/stanfordnlp_resources/en_ewt_models/en_ewt_tagger.pt', 'pretrain_path': '/root/stanfordnlp_resources/en_ewt_models/en_ewt.pretrain.pt', 'lang': 'en', 'shorthand': 'en_ewt', 'mode': 'predict'}\n",
            "---\n",
            "Loading: lemma\n",
            "With settings: \n",
            "{'model_path': '/root/stanfordnlp_resources/en_ewt_models/en_ewt_lemmatizer.pt', 'lang': 'en', 'shorthand': 'en_ewt', 'mode': 'predict'}\n",
            "Building an attentional Seq2Seq model...\n",
            "Using a Bi-LSTM encoder\n",
            "Using soft attention for LSTM.\n",
            "Finetune all embeddings.\n",
            "[Running seq2seq lemmatizer with edit classifier]\n",
            "---\n",
            "Loading: depparse\n",
            "With settings: \n",
            "{'model_path': '/root/stanfordnlp_resources/en_ewt_models/en_ewt_parser.pt', 'pretrain_path': '/root/stanfordnlp_resources/en_ewt_models/en_ewt.pretrain.pt', 'lang': 'en', 'shorthand': 'en_ewt', 'mode': 'predict'}\n",
            "Done loading processors!\n",
            "---\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F6KBg23x7X5J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# p = requests.get(\"https://en.wikipedia.org/wiki/Category:20th-century_British_women_writers\")\n",
        "# soup = BeautifulSoup(p.content, \"html.parser\")\n",
        "# authors = soup.findAll(\"div\", { \"class\" : \"mw-category\" })[0].text\n",
        "# auth_list = [i for i in authors.split(\"\\n\") if len(i)>1]\n",
        "\n",
        "# new_url = p + \"&pagefrom=auth_list%2C+Priyanga%0APriyanga+Burford\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z3i7yrFbq3Rz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "books = [\"gables\", \"avonlea\", \"island\", \"dreams\", \"rilla\" ,\"ingleside\", \"poplars\", \"rainbow\"]\n",
        "urls = [\"https://www.gutenberg.org/files/45/45-0.txt\", \"https://www.gutenberg.org/files/47/47-0.txt\", \"https://www.gutenberg.org/files/51/51-0.txt\", \"http://www.gutenberg.org/cache/epub/544/pg544.txt\", \"http://www.gutenberg.org/cache/epub/3796/pg3796.txt\", \"http://gutenberg.net.au/ebooks01/0100281.txt\", \"http://gutenberg.net.au/ebooks01/0100251.txt\", \"http://www.gutenberg.org/files/5343/5343-0.txt\"]\n",
        "\n",
        "for i in range(len(books)):\n",
        "    page = requests.get(urls[i])\n",
        "    file = open(\"{}.txt\".format(books[i]) , \"w+\")\n",
        "    file.write(page.text)\n",
        "    file.close()\n",
        "\n",
        "file = open(\"gables.txt\", \"r\")\n",
        "gables = file.read()\n",
        "file = open(\"avonlea.txt\", \"r\")\n",
        "avonlea = file.read()\n",
        "file = open(\"island.txt\", \"r\")\n",
        "island = file.read()\n",
        "file = open(\"dreams.txt\", \"r\")\n",
        "dreams = file.read()\n",
        "file = open(\"rilla.txt\", \"r\")\n",
        "rilla = file.read()\n",
        "file = open(\"ingleside.txt\", \"r\")\n",
        "ingleside = file.read()\n",
        "file = open(\"poplars.txt\", \"r\")\n",
        "poplars = file.read()\n",
        "file = open(\"rainbow.txt\", \"r\")\n",
        "rainbow = file.read()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PioYjUj5bAz8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bMMhVE0znZ8d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tnc1YLXPjqSg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#agent\n",
        "\n",
        "def return_nsubj(book, character=\"Anne\"):\n",
        "  sentences_with_agent_Anne = []\n",
        "  sentences_with_agent = []\n",
        "  if book==\"gables\":\n",
        "    doc = nlp(gables)\n",
        "  elif book==\"dreams\":\n",
        "    doc = nlp(dreams)\n",
        "  elif book==\"island\":\n",
        "    doc = nlp(island)\n",
        "  elif book==\"avonlea\":\n",
        "    doc = nlp(avonlea)\n",
        "  elif book==\"rilla\":\n",
        "    doc = nlp(rilla)\n",
        "  elif book==\"ingleside\":\n",
        "    doc = nlp(ingleside)\n",
        "  elif book==\"poplars\":\n",
        "    doc = nlp(poplars)\n",
        "  elif book==\"rainbow\":\n",
        "    doc = nlp(rainbow)\n",
        "  for i in list(doc.sentences):\n",
        "    for e in i.dependencies:\n",
        "  #       print(e[1])\n",
        "      if e[1]==\"nsubj\" or e[1]==\"agent\" :\n",
        "        if e[2].text==character:\n",
        "          sentences_with_agent_Anne.append(\" \".join([i.dependencies[a][2].text for a in range(len(i.dependencies)) if i.dependencies[a][2].text!=character]))\n",
        "        else:\n",
        "          sentences_with_agent.append(\" \".join([i.dependencies[a][2].text for a in range(len(i.dependencies)) if i.dependencies[a][2].text!=character]))\n",
        "        break\n",
        "  target = []\n",
        "  target.extend([[1] for i in range(len(sentences_with_agent_Anne))])\n",
        "  target.extend([[0] for i in range(len(sentences_with_agent))])\n",
        "  sentences = []\n",
        "  sentences.extend([[i] for i in sentences_with_agent_Anne])\n",
        "  sentences.extend([[i] for i in sentences_with_agent])\n",
        "  return sentences, target"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jxN0aA9uncBK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0d45261d-a4a8-45c3-ab77-5ac30695a447"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "os.chdir(\"/content/gdrive/My Drive/Colab Notebooks\")"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WwL8WyKoO5ld",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# full_para_list_gables, target_gables = return_nsubj(\"gables\")\n",
        "# full_para_list_dreams, target_dreams = return_nsubj(\"dreams\")\n",
        "# full_para_list_avonlea, target_avonlea = return_nsubj(\"avonlea\")\n",
        "# full_para_list_island, target_island = return_nsubj(\"island\")\n",
        "# full_para_list_rilla, target_rilla = return_nsubj(\"rilla\")\n",
        "# full_para_list_ingleside, target_ingleside= return_nsubj(\"ingleside\")\n",
        "# full_para_list_poplars, target_poplars = return_nsubj(\"poplars\")\n",
        "# full_para_list_rainbow, target_rainbow = return_nsubj(\"rainbow\")\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ZxehWGCnZJy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "ddd1f398-1ca0-4b73-89d0-594cac830e9a"
      },
      "source": [
        "os.listdir()"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Untitled0.ipynb',\n",
              " 'Untitled1.ipynb',\n",
              " 'Untitled2.ipynb',\n",
              " 'Copy of CS146 Ass3.ipynb',\n",
              " 'Untitled3.ipynb',\n",
              " 'CS146 Ass3.ipynb',\n",
              " 'cs146-7.1-pre-class-work.ipynb',\n",
              " 'Copy of Untitled0.ipynb',\n",
              " 'Copy of Rita CS152, 7.2.ipynb',\n",
              " 'Copy of 7.2Preclass JW.ipynb',\n",
              " 'Copy of CS146_LBA.ipynb',\n",
              " 'Untitled4.ipynb',\n",
              " 'Copy of Assignment 2 - Elisa Heinrich Mora',\n",
              " 'Copy of #2 drafts.ipynb',\n",
              " 'g.ipynb',\n",
              " 'Copy of Final Project - CS152',\n",
              " 'ExtraCS146.ipynb',\n",
              " 'CS146.ipynb',\n",
              " 'GPy.ipynb',\n",
              " 'Copy of WumpusWorldFinals.ipynb',\n",
              " 'CS166Assignment1.ipynb',\n",
              " 'Modified copy of CS166Assignment1.ipynb',\n",
              " 'Copy of 18 January copy of CS166Assignment1.ipynb',\n",
              " '18 January copy of CS166Assignment1.ipynb',\n",
              " 'Copy 12:25 pm on Jan 20 CS166Assignment1.ipynb',\n",
              " 'Untitled5.ipynb',\n",
              " 'CS166 2.2.ipynb',\n",
              " 'Untitled',\n",
              " 'Untitled6.ipynb',\n",
              " 'pyswip test notebook.ipynb',\n",
              " 'forest fire non-interactive.ipynb',\n",
              " 'Copy of forest fire non-interactive - with i (prob of catching on fire).ipynb',\n",
              " 'cs166 pre-class 4.1.ipynb',\n",
              " 'Copy of cs166 pre-class 4.1.ipynb',\n",
              " 'cs166 pre-class 4.2.ipynb',\n",
              " 'AH144 Charitable Giving Simulation.ipynb',\n",
              " 'CS166 5.2 NetworkX Code.ipynb',\n",
              " 'CS166 6.2.ipynb',\n",
              " 'Copy of CS166 6.2.ipynb',\n",
              " 'Untitled7.ipynb',\n",
              " 'CS166 Traffic Simulation (Assignment 2).ipynb',\n",
              " 'Untitled8.ipynb',\n",
              " 'CS166 7.1 PreClassWork.ipynb',\n",
              " 'Copy of CS166 7.1 PreClassWork.ipynb',\n",
              " 'CS166 7.2.ipynb',\n",
              " 'Capstone 8.1 Prototype Code.ipynb',\n",
              " 'CS166 8.1.ipynb',\n",
              " 'CS166 8.2.ipynb',\n",
              " 'workspace.ipynb',\n",
              " 'Untitled (1)',\n",
              " 'CS166 9.1.ipynb',\n",
              " 'CS166 10.1 .ipynb',\n",
              " 'CS166 10.2.ipynb',\n",
              " 'CS166 11.1.ipynb',\n",
              " 'cs166 12.1.ipynb',\n",
              " 'non.ipynb',\n",
              " 'CS166 12.2-pre-class-work - correct.ipynb',\n",
              " 'CS166 13.1 pcw.ipynb',\n",
              " 'CS166 13.2.ipynb',\n",
              " 'CS166 14.1 pcw.ipynb',\n",
              " 'Copy of Untitled1.ipynb',\n",
              " 'Untitled9.ipynb',\n",
              " 'Untitled10.ipynb',\n",
              " 'Untitled11.ipynb',\n",
              " 'Untitled12.ipynb',\n",
              " 'Untitled13.ipynb',\n",
              " 'Untitled14.ipynb',\n",
              " 'Copy of Untitled15.ipynb',\n",
              " 'fashion2 - sidegig.ipynb',\n",
              " 'fashion1.ipynb',\n",
              " 'Untitled15.ipynb',\n",
              " 'rico predicting perception. ipynb.ipynb',\n",
              " 'fashion 1 - prep data.ipynb',\n",
              " 'VAE for RICO.ipynb',\n",
              " 'Untitled17.ipynb',\n",
              " 'Untitled16.ipynb',\n",
              " 'PCW solution.ipynb',\n",
              " '__pycache__',\n",
              " 'CS142 Assignment 1.ipynb',\n",
              " 'CS142 3.2 pre-class work.ipynb',\n",
              " 'anno.ipynb',\n",
              " 'Capstone BME model.ipynb',\n",
              " 'dl 4.1 pcw.ipynb',\n",
              " 'Capstone - LL experiment.ipynb',\n",
              " 'gables.txt',\n",
              " 'avonlea.txt',\n",
              " 'island.txt',\n",
              " 'dreams.txt',\n",
              " 'rilla.txt',\n",
              " 'ingleside.txt',\n",
              " 'poplars.txt',\n",
              " 'rainbow.txt',\n",
              " 'anno_auto.ipynb',\n",
              " 'gables_sentence_list.txt',\n",
              " 'dreams_sentence_list.txt',\n",
              " 'avonlea_sentence_list.txt',\n",
              " 'island_sentence_list.txt',\n",
              " 'poplars_sentence_list.txt',\n",
              " 'rilla_sentence_list.txt',\n",
              " 'rainbow_sentence_list.txt',\n",
              " 'ingleside_sentence_list.txt']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eBNwaAjEysJZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# write to file\n",
        "file = open(\"gables_sentence_list.txt\", \"w\")\n",
        "for i in range(len(full_para_list_gables)):\n",
        "  file.write(full_para_list_gables[i][0])\n",
        "  file.write(\"\\n\")\n",
        "  file.write(str(target_gables[i][0]))\n",
        "  file.write(\"\\n\")\n",
        "file.close()\n",
        "  \n",
        "#write to file\n",
        "file = open(\"dreams_sentence_list.txt\", \"w\")\n",
        "for i in range(len(full_para_list_dreams)):\n",
        "  file.write(full_para_list_dreams[i][0])\n",
        "  file.write(\"\\n\")\n",
        "  file.write(str(target_dreams[i][0]))\n",
        "  file.write(\"\\n\")\n",
        "file.close()\n",
        "\n",
        "#write to file\n",
        "file = open(\"avonlea_sentence_list.txt\", \"w+\")\n",
        "for i in range(len(full_para_list_avonlea)):\n",
        "  file.write(full_para_list_avonlea[i][0])\n",
        "  file.write(\"\\n\")\n",
        "  file.write(str(target_avonlea[i][0]))\n",
        "  file.write(\"\\n\")\n",
        "file.close()\n",
        "\n",
        "file = open(\"island_sentence_list.txt\", \"w+\")\n",
        "for i in range(len(full_para_list_island)):\n",
        "  file.write(full_para_list_island[i][0])\n",
        "  file.write(\"\\n\")\n",
        "  file.write(str(target_island[i][0]))\n",
        "  file.write(\"\\n\")\n",
        "file.close()\n",
        "\n",
        "file = open(\"poplars_sentence_list.txt\", \"w+\")\n",
        "for i in range(len(full_para_list_poplars)):\n",
        "  file.write(full_para_list_poplars[i][0])\n",
        "  file.write(\"\\n\")\n",
        "  file.write(str(target_poplars[i][0]))\n",
        "  file.write(\"\\n\")\n",
        "file.close()\n",
        "\n",
        "file = open(\"rilla_sentence_list.txt\", \"w+\")\n",
        "for i in range(len(full_para_list_rilla)):\n",
        "  file.write(full_para_list_rilla[i][0])\n",
        "  file.write(\"\\n\")\n",
        "  file.write(str(target_rilla[i][0]))\n",
        "  file.write(\"\\n\")\n",
        "file.close()\n",
        "\n",
        "file = open(\"rainbow_sentence_list.txt\", \"w+\")\n",
        "for i in range(len(full_para_list_rainbow)):\n",
        "  file.write(full_para_list_rainbow[i][0])\n",
        "  file.write(\"\\n\")\n",
        "  file.write(str(target_rainbow[i][0]))\n",
        "  file.write(\"\\n\")\n",
        "file.close()\n",
        "\n",
        "file = open(\"ingleside_sentence_list.txt\", \"w+\")\n",
        "for i in range(len(full_para_list_ingleside)):\n",
        "  file.write(full_para_list_ingleside[i][0])\n",
        "  file.write(\"\\n\")\n",
        "  file.write(str(target_ingleside[i][0]))\n",
        "  file.write(\"\\n\")\n",
        "file.close()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "fY1Uf1CY5Uqt",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v1V0s7z4xYEd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}